# ur10e_reach_sac_part2.yaml
params:
  seed: 42

  env:
    clip_observations: 100.0
    clip_actions: 100.0

  algo:
    name: sac
  
  model:
    name: soft_actor_critic
    
  network:
    name: soft_actor_critic
    separate: True
    
    space:
      continuous:
        mu_activation: None
        sigma_activation: None
        mu_init:
          name: default
        sigma_init:
          name: const_initializer
          val: 0  # Part II uses 0
        fixed_sigma: False  # CHANGED: SAC learns std
    
    mlp:
      units: [512, 256, 128]  # CHANGED: Larger network from Part II
      activation: elu  # CHANGED: Part II uses elu
      d2rl: False
      initializer:
        name: default
      regularizer:
        name: None
    
    log_std_bounds: [-20, 2]  # Part II range
  
  load_checkpoint: False
  load_path: ''
  
  config:
    name: ur10e_reach_sac_part2
    env_name: rlgpu
    device: 'cuda:0'
    device_name: 'cuda:0'
    multi_gpu: False
    mixed_precision: False
    normalize_input: True
    normalize_value: False  # Not used in SAC
    num_actors: -1  # Use all available envs
    
    # Part II hyperparameters for speed
    gamma: 0.983  # CHANGED: Lower from Part II (was 0.99)
    tau: 0.0023  # CHANGED: Part II value (was 0.005)
    critic_tau: 0.0023  # Same as tau
    
    # Learning rates - Part II
    learning_rate: 0.00045  # Part II value
    actor_lr: 0.00045
    critic_lr: 0.00045
    alpha_lr: 0.00045  # entropy_learning_rate
    
    # CRITICAL: Initial alpha from Part II
    init_alpha: 0.0095  # CHANGED: Part II value (was 1.0)
    learnable_temperature: True
    
    # Batch and gradient steps - Part II
    batch_size: 512  # Part II value
    num_steps_per_episode: 1  # Collect 1 step before update
    steps_num: 32  # CHANGED: 32 gradient steps (Part II)
    
    # Replay buffer
    replay_buffer_size: 2000000  # CHANGED: Part II uses 2M
    
    # Warmup
    num_seed_steps: 0  # Use random_timesteps instead
    num_warmup_steps: 2000  # CHANGED: Part II learning_starts
    
    # Training schedule
    max_epochs: 100000
    max_frames: -1
    horizon_length: 1  # Not used in off-policy
    
    save_best_after: 1000
    save_frequency: 5000
    print_stats: True
    use_diagnostics: True

  
    grad_norm: 0  # Part II doesn't use grad clipping
    truncate_grads: False
    
    reward_shaper:
      scale_value: 1.0  # Don't scale rewards

  experiment:
    directory: "runs/ur10e_sac_part2"
    full_experiment_name: ""  # Will be auto-generated with timestamp
    train_dir: ""  # Will be set by training script
    experiment_name: ""
    write_interval: 1
    wandb: False
    wandb_kwargs:
      entity: your_entity
      project: ur10e_reach
      name: sac_part2_2048envs
      sync_tensorboard: True
  
    # Player config for evaluation
    player:
      deterministic: True
      games_num: 100
      print_stats: True